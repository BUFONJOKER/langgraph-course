# ğŸ¤– LLM Question & Answer - Sequential Workflow

A simple LangGraph workflow demonstrating LLM integration with sequential execution.

## ğŸ¯ Overview

Basic question-answering system using Large Language Models (Ollama or Hugging Face) in a sequential workflow.

## ğŸ”„ Workflow

```
START â†’ llm_qa â†’ END
```

Simple single-node workflow that processes a question through an LLM and returns the answer.

## ğŸš€ Usage

```bash
python main.py
```

**Example**:
```python
# Input question
question = "What is the capital of France?"

# Output
{
    'question': 'What is the capital of France?',
    'answer': 'The capital of France is Paris...'
}
```

## ğŸ“ Files

- **`llm_question_answer.py`** - LLM invocation node that processes questions
- **`llm_state_class.py`** - Pydantic state schema for question/answer
- **`model_ollama.py`** - Ollama model configuration
- **`model_huggingface.py`** - Hugging Face model configuration
- **`main.py`** - Main workflow setup and execution
- **`main.ipynb`** - Jupyter notebook interface

## ğŸ”§ Model Configuration

### Ollama
```python
from model_ollama import load_model
model = load_model()  # Uses local Ollama
```

### Hugging Face
```python
from model_huggingface import load_model
model = load_model()  # Uses HF models
```

## ğŸ“ Learning Points

- âœ… LLM integration with LangGraph
- âœ… Model abstraction with interchangeable backends
- âœ… Basic prompt construction
- âœ… State management for Q&A systems

## ğŸ“¦ Dependencies

```bash
pip install langgraph pydantic langchain-ollama
# OR
pip install langgraph pydantic langchain-huggingface
```

## âš™ï¸ Prerequisites

- Ollama installed and running (for Ollama backend)
- OR Hugging Face API key (for HF backend)

---

**Part of**: LangGraph Course - Sequential Workflows  
**Type**: LLM-Based Workflow
